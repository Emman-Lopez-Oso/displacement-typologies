{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from pyproj import Proj\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Denvercensus_90_10.csv' does not exist: b'Denvercensus_90_10.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-14843cf63105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshp_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'shp/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcity_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata_1990\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'census_90_10.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata_2000\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'census_00_10.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0macs_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'census_summ.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Denvercensus_90_10.csv' does not exist: b'Denvercensus_90_10.csv'"
     ]
    }
   ],
   "source": [
    "city_name = 'Denver'\n",
    "\n",
    "shp_folder = 'shp/'+city_name+'/'\n",
    "\n",
    "data_1990 = pd.read_csv(city_name+'census_90_10.csv', index_col = 0) \n",
    "data_2000 = pd.read_csv(city_name+'census_00_10.csv', index_col = 0)\n",
    "acs_data = pd.read_csv(city_name+'census_summ.csv', index_col = 0)\n",
    "\n",
    "acs_data = acs_data.drop(columns = ['county_y', 'state_y', 'tract_y'])\n",
    "acs_data = acs_data.rename(columns = {'county_x': 'county',\n",
    "                                    'state_x': 'state',\n",
    "                                    'tract_x': 'tract'})\n",
    "\n",
    "\n",
    "### PUMS\n",
    "pums_r = pd.read_csv('nhgis0002_ds233_20175_2017_tract.csv', encoding = \"ISO-8859-1\")\n",
    "pums_o = pd.read_csv('nhgis0002_ds234_20175_2017_tract.csv', encoding = \"ISO-8859-1\")\n",
    "pums = pums_r.merge(pums_o, on = 'GISJOIN')\n",
    "pums = pums.rename(columns = {'YEAR_x':'YEAR',\n",
    "                               'STATE_x':'STATE',\n",
    "                               'STATEA_x':'STATEA',\n",
    "                               'COUNTY_x':'COUNTY',\n",
    "                               'COUNTYA_x':'COUNTYA',\n",
    "                               'TRACTA_x':'TRACTA',\n",
    "                               'NAME_E_x':'NAME_E'})\n",
    "pums = pums.dropna(axis = 1)\n",
    "\n",
    "\n",
    "### Zillow data\n",
    "zillow = pd.read_csv('Zip_Zhvi_AllHomes.csv', encoding = \"ISO-8859-1\")\n",
    "zillow_xwalk = pd.read_csv('TRACT_ZIP_032015.csv')\n",
    "\n",
    "### Rail data\n",
    "rail = pd.read_csv('tod_database_download.csv')\n",
    "\n",
    "### Hospitals\n",
    "hospitals = pd.read_csv('Hospitals.csv')\n",
    "\n",
    "### Universities\n",
    "university = pd.read_csv('university_HD2016.csv')\n",
    "### LIHTC\n",
    "lihtc = pd.read_csv('LowIncome_Housing_Tax_Credit_Properties.csv')\n",
    "\n",
    "### Public housing\n",
    "pub_hous = pd.read_csv('Public_Housing_Buildings.csv')\n",
    "\n",
    "### SHP data\n",
    "if city_name == 'Memphis':\n",
    "    shp_name = 'cb_2017_47_tract_500k.shp'\n",
    "elif city_name == 'Chicago':\n",
    "    shp_name = 'cb_2017_17_tract_500k.shp'\n",
    "elif city_name == 'Atlanta':\n",
    "    shp_name = 'cb_2017_13_tract_500k.shp'\n",
    "elif city_name == 'Denver':\n",
    "    shp_name = 'cb_2017_08_tract_500k.shp'\n",
    "city_shp = gpd.read_file(shp_folder+shp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose city and define city specific variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if city_name == 'Chicago':\n",
    "    state = '17'\n",
    "    state_init = ['IL']\n",
    "    FIPS = ['031', '043', '089', '093', '097', '111', '197']\n",
    "    rail_agency = 'CTA'\n",
    "    zone = '16T'\n",
    "    \n",
    "elif city_name == 'Atlanta':\n",
    "    state = '13'\n",
    "    state_init = ['GA']\n",
    "    FIPS = ['057', '063', '067', '089', '097', '113', '121', '135', '151', '247']\n",
    "    rail_agency = 'MARTA'\n",
    "    zone = '16S'\n",
    "    \n",
    "elif city_name == 'Denver':\n",
    "    state = '08'\n",
    "    state_init = ['CO']\n",
    "    FIPS = ['001', '005', '013', '014', '019', '031', '035', '047', '059']\n",
    "    rail_agency = 'RTD'\n",
    "    zone = '13S'\n",
    "    \n",
    "elif city_name == 'Memphis':\n",
    "    state = ['28', '47']\n",
    "    state_init = ['MS', 'TN']\n",
    "    FIPS = {'28':['033', '093'], '47': ['047', '157']}\n",
    "    rail_agency = np.nan\n",
    "    zone = '15S'\n",
    "    \n",
    "else:\n",
    "    print ('There is no information for the selected city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge census data in single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = acs_data.merge(data_2000, on = 'FIPS', how = 'outer').merge(data_1990, on = 'FIPS', how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute census variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPI indexing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is based on the yearly CPI average\n",
    "CPI_89_17 = 1.977\n",
    "CPI_99_17 = 1.472\n",
    "\n",
    "### This is used for the Zillow data, where january values are compared\n",
    "CPI_0115_0119 = 1.077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census['hinc_17'][census['hinc_17']<0]=np.nan\n",
    "census['hinc_00'][census['hinc_00']<0]=np.nan\n",
    "census['hinc_90'][census['hinc_90']<0]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### These are not indexed\n",
    "rm_hinc_17 = np.nanmedian(census['hinc_17'])\n",
    "rm_hinc_00 = np.nanmedian(census['hinc_00'])\n",
    "rm_hinc_90 = np.nanmedian(census['hinc_90'])\n",
    "\n",
    "rm_iinc_17 = np.nanmedian(census['iinc_17'])\n",
    "rm_iinc_12 = np.nanmedian(census['iinc_12'])\n",
    "\n",
    "print(rm_hinc_17, rm_hinc_00, rm_hinc_90, rm_iinc_17, rm_iinc_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_interpolation (census, year, cutoff, mhinc, tot_var, var_suffix, out):\n",
    "    \n",
    "    name = []\n",
    "    for c in list(census.columns):\n",
    "        if (c[0]==var_suffix):\n",
    "            if c.split('_')[2]==year:\n",
    "                name.append(c)\n",
    "    name.append('FIPS')\n",
    "    name.append(tot_var)\n",
    "    \n",
    "    income_cat = census[name]\n",
    "    income_group = income_cat.drop(columns = ['FIPS', tot_var]).columns\n",
    "    income_group = income_group.str.split('_')\n",
    "    number = []\n",
    "    for i in range (0, len(income_group)):\n",
    "        number.append(income_group[i][1])\n",
    "\n",
    "    column = []\n",
    "    for i in number:\n",
    "        column.append('prop_'+str(i))\n",
    "        income_cat['prop_'+str(i)] = income_cat[var_suffix+'_'+str(i)+'_'+year]/income_cat[tot_var]\n",
    "             \n",
    "    reg_median_cutoff = cutoff*mhinc\n",
    "    cumulative = out+str(int(cutoff*100))+'_cumulative'\n",
    "    income = out+str(int(cutoff*100))+'_'+year\n",
    "    \n",
    "    df = income_cat\n",
    "    df[cumulative] = 0\n",
    "    df[income] = 0\n",
    "\n",
    "    for i in range(0,(len(number)-1)):\n",
    "        a = (number[i])\n",
    "        b = float(number[i+1])-0.01\n",
    "        prop = str(number[i+1])\n",
    "\n",
    "        df[cumulative] = df[cumulative]+df['prop_'+a]\n",
    "\n",
    "        if (reg_median_cutoff>=int(a))&(reg_median_cutoff<b):\n",
    "            df[income] = ((reg_median_cutoff - int(a))/(b-int(a)))*df['prop_'+prop] + df[cumulative]\n",
    "    \n",
    "    df = df.drop(columns = [cumulative])\n",
    "    prop_col = df.columns[df.columns.str[0:4]=='prop'] \n",
    "    df = df.drop(columns = prop_col)     \n",
    "\n",
    "    census = census.merge (df[['FIPS', income]], on = 'FIPS')\n",
    "    return census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = income_interpolation (census, '17', 0.8, rm_hinc_17, 'hh_17', 'I', 'inc')\n",
    "census =income_interpolation (census, '17', 1.2, rm_hinc_17, 'hh_17', 'I', 'inc')\n",
    "census = income_interpolation (census, '00', 0.8, rm_hinc_00, 'hh_00', 'I', 'inc')\n",
    "census =income_interpolation (census, '00', 1.2, rm_hinc_00, 'hh_00', 'I', 'inc')\n",
    "census = income_interpolation (census, '90', 0.8, rm_hinc_90, 'hh_00', 'I', 'inc')\n",
    "\n",
    "income_col = census.columns[census.columns.str[0:2]=='I_'] \n",
    "census = census.drop(columns = income_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Generate income categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_categories (df, year, mhinc, hinc):\n",
    "\n",
    "    df['hinc_'+year] = np.where(df['hinc_'+year]<0, 0, df['hinc_'+year])\n",
    "    \n",
    "    reg_med_inc80 = 0.8*mhinc\n",
    "    reg_med_inc120 = 1.2*mhinc\n",
    "    \n",
    "    low = 'low_80120_'+year \n",
    "    mod = 'mod_80120_'+year\n",
    "    high = 'high_80120_'+year\n",
    "\n",
    "    df[low] = df['inc80_'+year]\n",
    "    df[mod] = df['inc120_'+year] - df['inc80_'+year]\n",
    "    df[high] = 1 - df['inc120_'+year]\n",
    "    \n",
    "    ### Low income\n",
    "    df['low_pdmt_medhhinc_'+year] = np.where((df['low_80120_'+year]>=0.55)&(df['mod_80120_'+year]<0.45)&(df['high_80120_'+year]<0.45),1,0)\n",
    "\n",
    "    ## High income\n",
    "    df['high_pdmt_medhhinc_'+year] = np.where((df['low_80120_'+year]<0.45)&(df['mod_80120_'+year]<0.45)&(df['high_80120_'+year]>=0.55),1,0)\n",
    "\n",
    "    ### Moderate income\n",
    "    df['mod_pdmt_medhhinc_'+year] = np.where((df['low_80120_'+year]<0.45)&(df['mod_80120_'+year]>=0.55)&(df['high_80120_'+year]<0.45),1,0)\n",
    "\n",
    "    ### Mixed-Low income\n",
    "    df['mix_low_medhhinc_'+year] = np.where((df['low_pdmt_medhhinc_'+year]==0)&\n",
    "                                                  (df['mod_pdmt_medhhinc_'+year]==0)&\n",
    "                                                  (df['high_pdmt_medhhinc_'+year]==0)&\n",
    "                                                  (df[hinc]<reg_med_inc80),1,0)\n",
    "\n",
    "    ### Mixed-Moderate income\n",
    "    df['mix_mod_medhhinc_'+year] = np.where((df['low_pdmt_medhhinc_'+year]==0)&\n",
    "                                                  (df['mod_pdmt_medhhinc_'+year]==0)&\n",
    "                                                  (df['high_pdmt_medhhinc_'+year]==0)&\n",
    "                                                  (df[hinc]>=reg_med_inc80)&\n",
    "                                                  (df[hinc]<reg_med_inc120),1,0)\n",
    "\n",
    "    ### Mixed-High income\n",
    "    df['mix_high_medhhinc_'+year] = np.where((df['low_pdmt_medhhinc_'+year]==0)&\n",
    "                                                  (df['mod_pdmt_medhhinc_'+year]==0)&\n",
    "                                                  (df['high_pdmt_medhhinc_'+year]==0)&\n",
    "                                                  (df[hinc]>=reg_med_inc120),1,0)\n",
    "    \n",
    "    df['inc_cat_medhhinc_'+year] = 0\n",
    "    df.loc[df['low_pdmt_medhhinc_'+year]==1, 'inc_cat_medhhinc_'+year] = 1\n",
    "    df.loc[df['mix_low_medhhinc_'+year]==1, 'inc_cat_medhhinc_'+year] = 2\n",
    "    df.loc[df['mod_pdmt_medhhinc_'+year]==1, 'inc_cat_medhhinc_'+year] = 3\n",
    "    df.loc[df['mix_mod_medhhinc_'+year]==1, 'inc_cat_medhhinc_'+year] = 4\n",
    "    df.loc[df['mix_high_medhhinc_'+year]==1, 'inc_cat_medhhinc_'+year] = 5\n",
    "    df.loc[df['high_pdmt_medhhinc_'+year]==1, 'inc_cat_medhhinc_'+year] = 6\n",
    "    \n",
    "    df['inc_cat_medhhinc_encoded'+year] = 0\n",
    "    df.loc[df['low_pdmt_medhhinc_'+year]==1, 'inc_cat_medhhinc_encoded'+year] = 'low_pdmt'\n",
    "    df.loc[df['mix_low_medhhinc_'+year]==1, 'inc_cat_medhhinc_encoded'+year] = 'mix_low'\n",
    "    df.loc[df['mod_pdmt_medhhinc_'+year]==1, 'inc_cat_medhhinc_encoded'+year] = 'mod_pdmt'\n",
    "    df.loc[df['mix_mod_medhhinc_'+year]==1, 'inc_cat_medhhinc_encoded'+year] = 'mix_mod'\n",
    "    df.loc[df['mix_high_medhhinc_'+year]==1, 'inc_cat_medhhinc_encoded'+year] = 'mix_high'\n",
    "    df.loc[df['high_pdmt_medhhinc_'+year]==1, 'inc_cat_medhhinc_encoded'+year] = 'high_pdmt'\n",
    "\n",
    "    \n",
    "    df.loc[df['hinc_'+year]==0, 'low_pdmt_medhhinc_'+year] = np.nan\n",
    "    df.loc[df['hinc_'+year]==0, 'mix_low_medhhinc_'+year] = np.nan\n",
    "    df.loc[df['hinc_'+year]==0, 'mod_pdmt_medhhinc_'+year] = np.nan\n",
    "    df.loc[df['hinc_'+year]==0, 'mix_mod_medhhinc_'+year] = np.nan\n",
    "    df.loc[df['hinc_'+year]==0, 'mix_high_medhhinc_'+year] = np.nan\n",
    "    df.loc[df['hinc_'+year]==0, 'high_pdmt_medhhinc_'+year] = np.nan\n",
    "    df.loc[df['hinc_'+year]==0, 'inc_cat_medhhinc_'+year] = np.nan\n",
    "    \n",
    "    return census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = income_categories(census, '17', rm_hinc_17, 'hinc_17')\n",
    "census = income_categories(census, '00', rm_hinc_00, 'hinc_00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.groupby('inc_cat_medhhinc_00').count()['FIPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.groupby('inc_cat_medhhinc_17').count()['FIPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Percentage & total low-income households - under 80% AMI\n",
    "census ['per_all_li_90'] = census['inc80_90']\n",
    "census ['per_all_li_00'] = census['inc80_00']\n",
    "census ['per_all_li_17'] = census['inc80_17']\n",
    "\n",
    "census['all_li_count_90'] = census['per_all_li_90']*census['hh_90']\n",
    "census['all_li_count_00'] = census['per_all_li_00']*census['hh_00']\n",
    "census['all_li_count_17'] = census['per_all_li_17']*census['hh_17']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index all values to 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census['real_mhval_90'] = census['mhval_90']*CPI_89_17\n",
    "census['real_mrent_90'] = census['mrent_90']*CPI_89_17\n",
    "census['real_hinc_90'] = census['hinc_90']*CPI_89_17\n",
    "\n",
    "census['real_mhval_00'] = census['mhval_00']*CPI_99_17\n",
    "census['real_mrent_00'] = census['mrent_00']*CPI_99_17\n",
    "census['real_hinc_00'] = census['hinc_00']*CPI_99_17\n",
    "\n",
    "census['real_mhval_17'] = census['mhval_17']\n",
    "census['real_mrent_17'] = census['mrent_17']\n",
    "census['real_hinc_17'] = census['hinc_17']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = census\n",
    "\n",
    "### % of non-white\n",
    "\n",
    "###\n",
    "df['per_nonwhite_17'] = 1 - df['white_17']/df['pop_17']\n",
    "\n",
    "### 1990\n",
    "df['per_nonwhite_90'] = 1 - df['white_90']/df['pop_90']\n",
    "\n",
    "### 2000\n",
    "df['per_nonwhite_00'] = 1 - df['white_00']/df['pop_00']\n",
    "\n",
    "\n",
    "### % of owner and renter-occupied housing units\n",
    "### 1990\n",
    "df['hu_90'] = df['ohu_90']+df['rhu_90']\n",
    "df['per_rent_90'] = df['rhu_90']/df['hu_90']\n",
    "\n",
    "### 2000\n",
    "df['per_rent_00'] = df['rhu_00']/df['hu_00']\n",
    "\n",
    "\n",
    "### % of college educated\n",
    "\n",
    "### 1990\n",
    "var_list = ['total_25_col_9th_90',\n",
    "            'total_25_col_12th_90',\n",
    "            'total_25_col_hs_90',\n",
    "            'total_25_col_sc_90',\n",
    "            'total_25_col_ad_90',\n",
    "            'total_25_col_bd_90',\n",
    "            'total_25_col_gd_90']\n",
    "df['total_25_90'] = df[var_list].sum(axis = 1)\n",
    "df['per_col_90'] = (df['total_25_col_bd_90']+df['total_25_col_gd_90'])/(df['total_25_90'])\n",
    "\n",
    "### 2000\n",
    "df['male_25_col_00'] = (df['male_25_col_bd_00']+\n",
    "                        df['male_25_col_md_00']+\n",
    "#                         df['male_25_col_psd_00']+\n",
    "                        df['male_25_col_phd_00'])\n",
    "df['female_25_col_00'] = (df['female_25_col_bd_00']+\n",
    "                          df['female_25_col_md_00']+\n",
    "#                           df['female_25_col_psd_00']+\n",
    "                          df['female_25_col_phd_00'])\n",
    "df['total_25_col_00'] = df['male_25_col_00']+df['female_25_col_00']\n",
    "df['per_col_00'] = df['total_25_col_00']/df['total_25_00']\n",
    "\n",
    "### 2017\n",
    "df['per_col_17'] = (df['total_25_col_bd_17']+\n",
    "                    df['total_25_col_md_17']+\n",
    "                    df['total_25_col_pd_17']+\n",
    "                    df['total_25_col_phd_17'])/df['total_25_17']\n",
    "\n",
    "### Housing units built\n",
    "df['per_units_pre50_17'] = (df['units_40_49_built_17']+df['units_39_early_built_17'])/df['tot_units_built_17']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percent of people who have moved who are low-income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_interpolation_movein (census, year, cutoff, rm_iinc):\n",
    "    \n",
    "    # SUM EVERY CATEGORY BY INCOME\n",
    "\n",
    "    ### Filter only move-in variables\n",
    "    name = []\n",
    "    for c in list(census.columns):\n",
    "        if (c[0:3] == 'mov') & (c[-2:]==year):\n",
    "            name.append(c)\n",
    "    name.append('FIPS')\n",
    "    income_cat = census[name]\n",
    "\n",
    "    ### Pull income categories\n",
    "    income_group = income_cat.drop(columns = ['FIPS']).columns\n",
    "    number = []\n",
    "    for c in name[:9]:\n",
    "        number.append(c.split('_')[2])\n",
    "\n",
    "    ### Sum move-in in last 5 years by income category, including total w/ income\n",
    "    column_name_totals = []\n",
    "    for i in number:\n",
    "        column_name = []\n",
    "        for j in income_group:\n",
    "            if j.split('_')[2] == i:\n",
    "                column_name.append(j)\n",
    "        if i == 'w':\n",
    "            i = 'w_income'\n",
    "        income_cat['mov_tot_'+i+'_'+year] = income_cat[column_name].sum(axis = 1)\n",
    "        column_name_totals.append('mov_tot_'+i+'_'+year)\n",
    "\n",
    "    # DO INCOME INTERPOLATION\n",
    "    column = []\n",
    "    number = [n for n in number if n != 'w'] ### drop total\n",
    "    for i in number:\n",
    "        column.append('prop_mov_'+i)\n",
    "        income_cat['prop_mov_'+i] = income_cat['mov_tot_'+i+'_'+year]/income_cat['mov_tot_w_income_'+year]\n",
    "\n",
    "\n",
    "    reg_median_cutoff = cutoff*rm_iinc\n",
    "    cumulative = 'inc'+str(int(cutoff*100))+'_cumulative'\n",
    "    per_limove = 'per_limove_'+year\n",
    "\n",
    "    df = income_cat\n",
    "    df[cumulative] = 0\n",
    "    df[per_limove] = 0\n",
    "\n",
    "    for i in range(0,(len(number)-1)):\n",
    "        a = (number[i])\n",
    "        b = float(number[i+1])-0.01\n",
    "        prop = str(number[i+1])\n",
    "\n",
    "        df[cumulative] = df[cumulative]+df['prop_mov_'+a]\n",
    "\n",
    "        if (reg_median_cutoff>=int(a))&(reg_median_cutoff<b):\n",
    "            df[per_limove] = ((reg_median_cutoff - int(a))/(b-int(a)))*df['prop_mov_'+prop] + df[cumulative]\n",
    "            \n",
    "    df = df.drop(columns = [cumulative])\n",
    "    prop_col = df.columns[df.columns.str[0:4]=='prop'] \n",
    "    df = df.drop(columns = prop_col)     \n",
    "\n",
    "    col_list = [per_limove]+['mov_tot_w_income_'+year]\n",
    "    census = census.merge (df[['FIPS'] + col_list], on = 'FIPS')\n",
    "    return census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = income_interpolation_movein (census, '17', 0.8, rm_iinc_17)\n",
    "census = income_interpolation_movein (census, '12', 0.8, rm_iinc_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Housing Affordability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_PUMS(df, FIPS):\n",
    "    if city_name != 'Memphis':\n",
    "        FIPS = [int(x) for x in FIPS]\n",
    "        df = df[(df['STATEA'] == int(state))&(df['COUNTYA'].isin(FIPS))].reset_index(drop = True)\n",
    "\n",
    "    else:\n",
    "        fips_list = []\n",
    "        for i in state:\n",
    "            county = FIPS[i]\n",
    "            county = [int(x) for x in county]\n",
    "            a = list((df['GISJOIN'][(pums['STATEA']==int(i))&(df['COUNTYA'].isin(county))]))\n",
    "            fips_list += a\n",
    "        df = df[df['GISJOIN'].isin(fips_list)].reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pums = filter_PUMS(pums, FIPS)\n",
    "pums['FIPS'] = ((pums['STATEA'].astype(str).str.zfill(2))+\n",
    "                (pums['COUNTYA'].astype(str).str.zfill(3))+\n",
    "                (pums['TRACTA'].astype(str).str.zfill(6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pums = pums.rename(columns = {\"AH5QE002\":\"rhu_17_wcash\",\n",
    "                                \"AH5QE003\":\"R_100_17\",\n",
    "                                \"AH5QE004\":\"R_150_17\",\n",
    "                                \"AH5QE005\":\"R_200_17\",\n",
    "                                \"AH5QE006\":\"R_250_17\",\n",
    "                                \"AH5QE007\":\"R_300_17\",\n",
    "                                \"AH5QE008\":\"R_350_17\",\n",
    "                                \"AH5QE009\":\"R_400_17\",\n",
    "                                \"AH5QE010\":\"R_450_17\",\n",
    "                                \"AH5QE011\":\"R_500_17\",\n",
    "                                \"AH5QE012\":\"R_550_17\",\n",
    "                                \"AH5QE013\":\"R_600_17\",\n",
    "                                \"AH5QE014\":\"R_650_17\",\n",
    "                                \"AH5QE015\":\"R_700_17\",\n",
    "                                \"AH5QE016\":\"R_750_17\",\n",
    "                                \"AH5QE017\":\"R_800_17\",\n",
    "                                \"AH5QE018\":\"R_900_17\",\n",
    "                                \"AH5QE019\":\"R_1000_17\",\n",
    "                                \"AH5QE020\":\"R_1250_17\",\n",
    "                                \"AH5QE021\":\"R_1500_17\",\n",
    "                                \"AH5QE022\":\"R_2000_17\",\n",
    "                                \"AH5QE023\":\"R_2500_17\",\n",
    "                                \"AH5QE024\":\"R_3000_17\",\n",
    "                                \"AH5QE025\":\"R_3500_17\",\n",
    "                                \"AH5QE026\":\"R_3600_17\",\n",
    "                                \"AH5QE027\":\"rhu_17_wocash\",\n",
    "                                \"AIMUE001\":\"ohu_tot_17\",\n",
    "                                \"AIMUE002\":\"O_200_17\",\n",
    "                                \"AIMUE003\":\"O_300_17\",\n",
    "                                \"AIMUE004\":\"O_400_17\",\n",
    "                                \"AIMUE005\":\"O_500_17\",\n",
    "                                \"AIMUE006\":\"O_600_17\",\n",
    "                                \"AIMUE007\":\"O_700_17\",\n",
    "                                \"AIMUE008\":\"O_800_17\",\n",
    "                                \"AIMUE009\":\"O_900_17\",\n",
    "                                \"AIMUE010\":\"O_1000_17\",\n",
    "                                \"AIMUE011\":\"O_1250_17\",\n",
    "                                \"AIMUE012\":\"O_1500_17\",\n",
    "                                \"AIMUE013\":\"O_2000_17\",\n",
    "                                \"AIMUE014\":\"O_2500_17\",\n",
    "                                \"AIMUE015\":\"O_3000_17\",\n",
    "                                \"AIMUE016\":\"O_3500_17\",\n",
    "                                \"AIMUE017\":\"O_4000_17\",\n",
    "                                \"AIMUE018\":\"O_4100_17\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_17 = rm_hinc_17*0.3/12\n",
    "pums = income_interpolation (pums, '17', 0.6, aff_17, 'rhu_17_wcash', 'R', 'rent')\n",
    "pums = income_interpolation (pums, '17', 1.2, aff_17, 'rhu_17_wcash', 'R', 'rent')\n",
    "\n",
    "pums = income_interpolation (pums, '17', 0.6, aff_17, 'ohu_tot_17', 'O', 'own')\n",
    "pums = income_interpolation (pums, '17', 1.2, aff_17, 'ohu_tot_17', 'O', 'own')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pums['FIPS'] = pums['FIPS'].astype(float).astype('int64')\n",
    "pums = pums.merge(census[['FIPS', 'mmhcosts_17']], on = 'FIPS')\n",
    "\n",
    "pums['rlow_17'] = pums['rent60_17']*pums['rhu_17_wcash']+pums['rhu_17_wocash'] ### includes no cash rent\n",
    "pums['rmod_17'] = pums['rent120_17']*pums['rhu_17_wcash']-pums['rent60_17']*pums['rhu_17_wcash']\n",
    "pums['rhigh_17'] = pums['rhu_17_wcash']-pums['rent120_17']*pums['rhu_17_wcash']\n",
    "\n",
    "pums['olow_17'] = pums['own60_17']*pums['ohu_tot_17']\n",
    "pums['omod_17'] = pums['own120_17']*pums['ohu_tot_17'] - pums['own60_17']*pums['ohu_tot_17']\n",
    "pums['ohigh_17'] = pums['ohu_tot_17'] - pums['own120_17']*pums['ohu_tot_17']\n",
    "\n",
    "pums['hu_tot_17'] = pums['rhu_17_wcash']+pums['rhu_17_wocash']+pums['ohu_tot_17']\n",
    "\n",
    "pums['low_tot_17'] = pums['rlow_17']+pums['olow_17']\n",
    "pums['mod_tot_17'] = pums['rmod_17']+pums['omod_17']\n",
    "pums['high_tot_17'] = pums['rhigh_17']+pums['ohigh_17']\n",
    "\n",
    "pums['pct_low_17'] = pums['low_tot_17']/pums['hu_tot_17']\n",
    "pums['pct_mod_17'] = pums['mod_tot_17']/pums['hu_tot_17']\n",
    "pums['pct_high_17'] = pums['high_tot_17']/pums['hu_tot_17']\n",
    "\n",
    "\n",
    "### Low income\n",
    "pums['predominantly_LI'] = np.where((pums['pct_low_17']>=0.55)&\n",
    "                                       (pums['pct_mod_17']<0.45)&\n",
    "                                       (pums['pct_high_17']<0.45),1,0)\n",
    "\n",
    "## High income\n",
    "pums['predominantly_HI'] = np.where((pums['pct_low_17']<0.45)&\n",
    "                                       (pums['pct_mod_17']<0.45)&\n",
    "                                       (pums['pct_high_17']>=0.55),1,0)\n",
    "\n",
    "### Moderate income\n",
    "pums['predominantly_MI'] = np.where((pums['pct_low_17']<0.45)&\n",
    "                                       (pums['pct_mod_17']>=0.55)&\n",
    "                                       (pums['pct_high_17']<0.45),1,0)\n",
    "\n",
    "### Mixed-Low income\n",
    "pums['mixed_low'] = np.where((pums['predominantly_LI']==0)&\n",
    "                              (pums['predominantly_MI']==0)&\n",
    "                              (pums['predominantly_HI']==0)&\n",
    "                              (pums['mmhcosts_17']<aff_17*0.6),1,0)\n",
    "\n",
    "### Mixed-Moderate income\n",
    "pums['mixed_mod'] = np.where((pums['predominantly_LI']==0)&\n",
    "                              (pums['predominantly_MI']==0)&\n",
    "                              (pums['predominantly_HI']==0)&\n",
    "                              (pums['mmhcosts_17']>=aff_17*0.6)&\n",
    "                              (pums['mmhcosts_17']<aff_17*1.2),1,0)\n",
    "\n",
    "### Mixed-High income\n",
    "pums['mixed_high'] = np.where((pums['predominantly_LI']==0)&\n",
    "                              (pums['predominantly_MI']==0)&\n",
    "                              (pums['predominantly_HI']==0)&\n",
    "                              (pums['mmhcosts_17']>=aff_17*1.2),1,0)\n",
    "\n",
    "pums['lmh_flag_encoded'] = 0\n",
    "pums.loc[pums['predominantly_LI']==1, 'lmh_flag_encoded'] = 1\n",
    "pums.loc[pums['predominantly_MI']==1, 'lmh_flag_encoded'] = 2\n",
    "pums.loc[pums['predominantly_HI']==1, 'lmh_flag_encoded'] = 3\n",
    "pums.loc[pums['mixed_low']==1, 'lmh_flag_encoded'] = 4\n",
    "pums.loc[pums['mixed_mod']==1, 'lmh_flag_encoded'] = 5\n",
    "pums.loc[pums['mixed_high']==1, 'lmh_flag_encoded'] = 6\n",
    "\n",
    "pums['lmh_flag_category'] = 0\n",
    "pums.loc[pums['lmh_flag_encoded']==1, 'lmh_flag_category'] = 'aff_predominantly_LI'\n",
    "pums.loc[pums['lmh_flag_encoded']==2, 'lmh_flag_category'] = 'aff_predominantly_MI'\n",
    "pums.loc[pums['lmh_flag_encoded']==3, 'lmh_flag_category'] = 'aff_predominantly_HI'\n",
    "pums.loc[pums['lmh_flag_encoded']==4, 'lmh_flag_category'] = 'aff_mix_low'\n",
    "pums.loc[pums['lmh_flag_encoded']==5, 'lmh_flag_category'] = 'aff_mix_mod'\n",
    "pums.loc[pums['lmh_flag_encoded']==6, 'lmh_flag_category'] = 'aff_mix_high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pums.groupby('lmh_flag_category').count()['FIPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = census.merge(pums[['FIPS', 'lmh_flag_encoded', 'lmh_flag_category']], on = 'FIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Market Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census['pctch_real_mhval_00_17'] = (census['real_mhval_17']-census['real_mhval_00'])/census['real_mhval_00']\n",
    "census['pctch_real_mrent_00_17'] = (census['real_mrent_17']-census['real_mrent_00'])/census['real_mrent_00']\n",
    "\n",
    "rm_pctch_real_mhval_00_17_increase=np.nanmedian(census['pctch_real_mhval_00_17'][census['pctch_real_mhval_00_17']>0.05])\n",
    "rm_pctch_real_mrent_00_17_increase=np.nanmedian(census['pctch_real_mrent_00_17'][census['pctch_real_mrent_00_17']>0.05])\n",
    "\n",
    "# rm_pctch_real_mhval_00_17_increase=np.nanmedian(census['pctch_real_mhval_00_17'])\n",
    "# rm_pctch_real_mrent_00_17_increase=np.nanmedian(census['pctch_real_mrent_00_17'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census['rent_decrease'] = np.where((census['pctch_real_mrent_00_17']<=-0.05), 1, 0)\n",
    "\n",
    "census['rent_marginal'] = np.where((census['pctch_real_mrent_00_17']>-0.05)&\n",
    "                                          (census['pctch_real_mrent_00_17']<0.05), 1, 0)\n",
    "\n",
    "census['rent_increase'] = np.where((census['pctch_real_mrent_00_17']>=0.05)&\n",
    "                                          (census['pctch_real_mrent_00_17']<rm_pctch_real_mrent_00_17_increase), 1, 0)\n",
    "\n",
    "census['rent_rapid_increase'] = np.where((census['pctch_real_mrent_00_17']>=0.05)&\n",
    "                                          (census['pctch_real_mrent_00_17']>=rm_pctch_real_mrent_00_17_increase), 1, 0)\n",
    "\n",
    "\n",
    "census['house_decrease'] = np.where((census['pctch_real_mhval_00_17']<=-0.05), 1, 0)\n",
    "\n",
    "census['house_marginal'] = np.where((census['pctch_real_mhval_00_17']>-0.05)&\n",
    "                                          (census['pctch_real_mhval_00_17']<0.05), 1, 0)\n",
    "\n",
    "census['house_increase'] = np.where((census['pctch_real_mhval_00_17']>=0.05)&\n",
    "                                          (census['pctch_real_mhval_00_17']<rm_pctch_real_mhval_00_17_increase), 1, 0)\n",
    "\n",
    "census['house_rapid_increase'] = np.where((census['pctch_real_mhval_00_17']>=0.05)&\n",
    "                                          (census['pctch_real_mhval_00_17']>=rm_pctch_real_mhval_00_17_increase), 1, 0)\n",
    "\n",
    "census['tot_decrease'] = np.where((census['rent_decrease']==1)|(census['house_decrease']), 1, 0)\n",
    "census['tot_marginal'] = np.where((census['rent_marginal']==1)|(census['house_marginal']), 1, 0)\n",
    "census['tot_increase'] = np.where((census['rent_increase']==1)|(census['house_increase']), 1, 0)\n",
    "census['tot_rapid_increase'] = np.where((census['rent_rapid_increase']==1)|(census['house_rapid_increase']), 1, 0)\n",
    "\n",
    "census['change_flag_encoded'] = 0\n",
    "census.loc[(census['tot_decrease']==1)|(census['tot_marginal']==1), 'change_flag_encoded'] = 1\n",
    "census.loc[census['tot_increase']==1, 'change_flag_encoded'] = 2\n",
    "census.loc[census['tot_rapid_increase']==1, 'change_flag_encoded'] = 3\n",
    "\n",
    "census['change_flag_category'] = 0\n",
    "census.loc[census['change_flag_encoded']==1, 'change_flag_category'] = 'ch_decrease_marginal'\n",
    "census.loc[census['change_flag_encoded']==2, 'change_flag_category'] = 'ch_increase'\n",
    "census.loc[census['change_flag_encoded']==3, 'change_flag_category'] = 'ch_rapid_increase'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.groupby('change_flag_category').count()['FIPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.groupby(['change_flag_category', 'lmh_flag_category']).count()['FIPS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load Zillow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ZILLOW(df, FIPS):\n",
    "    if city_name != 'Memphis':\n",
    "        FIPS_pre = [state+county for county in FIPS]\n",
    "        df = df[(df['FIPS'].astype(str).str.zfill(11).str[:5].isin(FIPS_pre))].reset_index(drop = True)\n",
    "\n",
    "    else:\n",
    "        fips_list = []\n",
    "        for i in state:\n",
    "            county = FIPS[str(i)]\n",
    "            FIPS_pre = [str(i)+county for county in county]         \n",
    "        df = df[(df['FIPS'].astype(str).str.zfill(11).str[:5].isin(FIPS_pre))].reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Zillow data\n",
    "zillow = pd.read_csv('Zip_Zhvi_AllHomes.csv', encoding = \"ISO-8859-1\")\n",
    "zillow_xwalk = pd.read_csv('TRACT_ZIP_032015.csv')\n",
    "\n",
    "## Compute change over time\n",
    "zillow['ch_zillow_15_19'] = zillow['2019-01'] - zillow['2015-01']*CPI_0115_0119\n",
    "zillow['per_ch_zillow_15_19'] = zillow['ch_zillow_15_19']/zillow['2015-01']\n",
    "zillow = zillow[zillow['State'].isin(state_init)].reset_index(drop = True)\n",
    "zillow = zillow_xwalk[['TRACT', 'ZIP', 'RES_RATIO']].merge(zillow[['RegionName', 'ch_zillow_15_19', 'per_ch_zillow_15_19']], left_on = 'ZIP', right_on = 'RegionName', how = 'outer')\n",
    "zillow = zillow.rename(columns = {'TRACT':'FIPS'})\n",
    "\n",
    "# Filter only data of interest\n",
    "zillow = filter_ZILLOW(zillow, FIPS)\n",
    "\n",
    "### Keep only data for largest xwalk value, based on residential ratio\n",
    "zillow = zillow.sort_values(by = ['FIPS', 'RES_RATIO'], ascending = False).groupby('FIPS').first().reset_index(drop = False)\n",
    "\n",
    "### Compute 90th percentile change in region\n",
    "percentile_90 = zillow['per_ch_zillow_15_19'].quantile(q = 0.9)\n",
    "print(percentile_90)\n",
    "\n",
    "### Create flags\n",
    "### Change over 50% of change in region\n",
    "zillow['ab_50pct_ch'] = np.where(zillow['per_ch_zillow_15_19']>0.5, 1, 0)\n",
    "### Change over 90th percentile change\n",
    "zillow['ab_90percentile_ch'] = np.where(zillow['per_ch_zillow_15_19']>percentile_90, 1, 0)\n",
    "\n",
    "census = census.merge(zillow[['FIPS', 'ab_50pct_ch', 'ab_90percentile_ch']], on = 'FIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regional medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_per_all_li_90 = np.nanmedian(census['per_all_li_90'])\n",
    "rm_per_all_li_00 = np.nanmedian(census['per_all_li_00'])\n",
    "rm_per_nonwhite_90 = np.nanmedian(census['per_nonwhite_90'])\n",
    "rm_per_nonwhite_00 = np.nanmedian(census['per_nonwhite_00'])\n",
    "rm_per_nonwhite_17 = np.nanmedian(census['per_nonwhite_17'])\n",
    "rm_per_col_90 = np.nanmedian(census['per_col_90'])\n",
    "rm_per_col_00 = np.nanmedian(census['per_col_00'])\n",
    "rm_per_col_17 = np.nanmedian(census['per_col_17'])\n",
    "rm_per_rent_90= np.nanmedian(census['per_rent_90'])\n",
    "rm_per_rent_00= np.nanmedian(census['per_rent_00'])\n",
    "rm_real_mrent_90 = np.nanmedian(census['real_mrent_90'])\n",
    "rm_real_mrent_00 = np.nanmedian(census['real_mrent_00'])\n",
    "rm_real_mrent_17 = np.nanmedian(census['real_mrent_17'])\n",
    "rm_real_mhval_90 = np.nanmedian(census['real_mhval_90'])\n",
    "rm_real_mhval_00 = np.nanmedian(census['real_mhval_00'])\n",
    "rm_real_mhval_17 = np.nanmedian(census['real_mhval_17'])\n",
    "rm_real_hinc_90 = np.nanmedian(census['real_hinc_90'])\n",
    "rm_real_hinc_00 = np.nanmedian(census['real_hinc_00'])\n",
    "rm_real_hinc_17 = np.nanmedian(census['real_hinc_17'])\n",
    "rm_per_units_pre50_17 = np.nanmedian(census['per_units_pre50_17'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percent changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census['pctch_real_mhval_90_00'] = (census['real_mhval_00']-census['real_mhval_90'])/census['real_mhval_90']\n",
    "census['pctch_real_mrent_90_00'] = (census['real_mrent_00']-census['real_mrent_90'])/census['real_mrent_90']\n",
    "census['pctch_real_hinc_90_00'] = (census['real_hinc_00']-census['real_hinc_90'])/census['real_hinc_90']\n",
    "\n",
    "census['pctch_real_mhval_00_17'] = (census['real_mhval_17']-census['real_mhval_00'])/census['real_mhval_00']\n",
    "census['pctch_real_mrent_00_17'] = (census['real_mrent_17']-census['real_mrent_00'])/census['real_mrent_00']\n",
    "census['pctch_real_hinc_00_17'] = (census['real_hinc_17']-census['real_hinc_00'])/census['real_hinc_00']\n",
    "\n",
    "### Regional Medians\n",
    "pctch_rm_real_mhval_90_00 = (rm_real_mhval_00-rm_real_mhval_90)/rm_real_mhval_90\n",
    "pctch_rm_real_mrent_90_00 = (rm_real_mrent_00-rm_real_mrent_90)/rm_real_mrent_90\n",
    "pctch_rm_real_mhval_00_17 = (rm_real_mhval_17-rm_real_mhval_00)/rm_real_mhval_00\n",
    "pctch_rm_real_mrent_00_17 = (rm_real_mrent_17-rm_real_mrent_00)/rm_real_mrent_00\n",
    "pctch_rm_real_hinc_90_00 = (rm_real_hinc_00-rm_real_hinc_90)/rm_real_hinc_90\n",
    "pctch_rm_real_hinc_00_17 = (rm_real_hinc_17-rm_real_hinc_00)/rm_real_hinc_00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census['ch_all_li_count_90_00'] = census['all_li_count_00']-census['all_li_count_90']\n",
    "census['ch_all_li_count_00_17'] = census['all_li_count_17']-census['all_li_count_00']\n",
    "census['ch_per_col_90_00'] = census['per_col_00']-census['per_col_90']\n",
    "census['ch_per_col_00_17'] = census['per_col_17']-census['per_col_00']\n",
    "census['ch_per_limove_12_17'] = census['per_limove_17'] - census['per_limove_12']\n",
    "\n",
    "### Regional Medians\n",
    "ch_rm_per_col_90_00 = rm_per_col_00-rm_per_col_90\n",
    "ch_rm_per_col_00_17 = rm_per_col_17-rm_per_col_00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = census\n",
    "df['pop00flag'] = np.where(df['pop_00']>500, 1, 0)\n",
    "df['aboverm_per_all_li_90'] = np.where(df['per_all_li_90']>=rm_per_all_li_90, 1, 0)\n",
    "df['aboverm_per_all_li_00'] = np.where(df['per_all_li_00']>=rm_per_all_li_00, 1, 0)\n",
    "df['aboverm_per_nonwhite_17'] = np.where(df['per_nonwhite_17']>=rm_per_nonwhite_17, 1, 0)\n",
    "df['aboverm_per_nonwhite_90'] = np.where(df['per_nonwhite_90']>=rm_per_nonwhite_90, 1, 0)\n",
    "df['aboverm_per_nonwhite_00'] = np.where(df['per_nonwhite_00']>=rm_per_nonwhite_00, 1, 0)\n",
    "df['aboverm_per_rent_90'] = np.where(df['per_rent_90']>=rm_per_rent_90, 1, 0)\n",
    "df['aboverm_per_rent_00'] = np.where(df['per_rent_00']>=rm_per_rent_00, 1, 0)\n",
    "df['aboverm_per_col_90'] = np.where(df['per_col_90']>=rm_per_col_90, 1, 0)\n",
    "df['aboverm_per_col_00'] = np.where(df['per_col_00']>=rm_per_col_00, 1, 0)\n",
    "df['aboverm_per_col_17'] = np.where(df['per_col_17']>=rm_per_col_17, 1, 0)\n",
    "df['aboverm_real_mrent_90'] = np.where(df['real_mrent_90']>=rm_real_mrent_90, 1, 0)\n",
    "df['aboverm_real_mrent_00'] = np.where(df['real_mrent_00']>=rm_real_mrent_00, 1, 0)\n",
    "df['aboverm_real_mhval_90'] = np.where(df['real_mhval_90']>=rm_real_mhval_90, 1, 0)\n",
    "df['aboverm_real_mhval_00'] = np.where(df['real_mhval_00']>=rm_real_mhval_00, 1, 0)\n",
    "df['aboverm_pctch_real_mhval_00_17'] = np.where(df['pctch_real_mhval_00_17']>=pctch_rm_real_mhval_00_17, 1, 0)\n",
    "df['aboverm_pctch_real_mrent_00_17'] = np.where(df['pctch_real_mrent_00_17']>=pctch_rm_real_mrent_00_17, 1, 0)\n",
    "df['aboverm_pctch_real_mhval_90_00'] = np.where(df['pctch_real_mhval_90_00']>=pctch_rm_real_mhval_90_00, 1, 0)\n",
    "df['aboverm_pctch_real_mrent_90_00'] = np.where(df['pctch_real_mrent_90_00']>=pctch_rm_real_mrent_90_00, 1, 0)\n",
    "df['lostli_00'] = np.where(df['ch_all_li_count_90_00']<0, 1, 0)\n",
    "df['lostli_17'] = np.where(df['ch_all_li_count_00_17']<0, 1, 0)\n",
    "df['aboverm_pctch_real_hinc_90_00'] = np.where(df['pctch_real_hinc_90_00']>pctch_rm_real_hinc_90_00, 1, 0)\n",
    "df['aboverm_pctch_real_hinc_00_17'] = np.where(df['pctch_real_hinc_00_17']>pctch_rm_real_hinc_00_17, 1, 0)\n",
    "df['aboverm_ch_per_col_90_00'] = np.where(df['ch_per_col_90_00']>ch_rm_per_col_90_00, 1, 0)\n",
    "df['aboverm_ch_per_col_00_17'] = np.where(df['ch_per_col_00_17']>ch_rm_per_col_00_17, 1, 0)\n",
    "df['aboverm_per_units_pre50_17'] = np.where(df['per_units_pre50_17']>rm_per_units_pre50_17, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Analysis Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter only census tracts of interest from shp\n",
    "census_tract_list = census['FIPS'].astype(str).str.zfill(11)\n",
    "city_shp = city_shp[city_shp['GEOID'].isin(census_tract_list)].reset_index(drop = True)\n",
    "\n",
    "# Create subset of points for faster running\n",
    "### Create single region polygon\n",
    "city_poly = city_shp.dissolve(by = 'STATEFP')\n",
    "city_poly = city_poly.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Rail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter only existing rail\n",
    "rail = rail[rail['Year Opened']=='Pre-2000'].reset_index(drop = True)\n",
    "\n",
    "### Filter by city\n",
    "rail = rail[rail['Agency'] == rail_agency].reset_index(drop = True)\n",
    "rail = gpd.GeoDataFrame(rail, geometry=[Point(xy) for xy in zip (rail['Longitude'], rail['Latitude'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check whether census tract contains rail station\n",
    "### and create rail flag\n",
    "\n",
    "### Create half mile buffer\n",
    "\n",
    "### sets coordinate system to WGS84\n",
    "rail.crs = {'init' :'epsg:4269'}\n",
    "\n",
    "### creates UTM projection\n",
    "### zone is defined under define city specific variables\n",
    "projection = '+proj=utm +zone='+zone+', +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "\n",
    "### project to UTM coordinate system\n",
    "rail_proj = rail.to_crs(projection)\n",
    "\n",
    "### create buffer around anchor institution in meters\n",
    "rail_buffer = rail_proj.buffer(804.672)\n",
    "\n",
    "### convert buffer back to WGS84\n",
    "rail_buffer_wgs = rail_buffer.to_crs(epsg=4326)\n",
    "\n",
    "### crate flag\n",
    "city_shp['rail'] = np.where(city_shp.intersects(rail_buffer_wgs.unary_union) == True, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if city_name != 'Memphis':\n",
    "    ax = city_shp.plot('rail')\n",
    "    rail.plot(ax = ax, color = 'red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Anchor institution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hospitals\n",
    "hospitals = pd.read_csv('Hospitals.csv')\n",
    "\n",
    "### Universities\n",
    "university = pd.read_csv('university_HD2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter for state of interest\n",
    "hospitals = hospitals[hospitals['STATE'].isin(state_init)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert to geodataframe\n",
    "hospitals = gpd.GeoDataFrame(hospitals, geometry=[Point(xy) for xy in zip (hospitals['X'], hospitals['Y'])])\n",
    "\n",
    "### Filter to hospitals with 100+ beds\n",
    "hospitals = hospitals[hospitals['BEDS']>=100].reset_index(drop = True)\n",
    "hosp_type = [\"GENERAL MEDICAL AND SURGICAL HOSPITALS\", \"CHILDREN'S HOSPITALS, GENERAL\"]\n",
    "hospitals = hospitals[hospitals['NAICS_DESC'].isin(hosp_type)]\n",
    "hospitals = hospitals[['geometry']]\n",
    "hospitals = hospitals.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter for state of interest\n",
    "university = university[university['STABBR'].isin(state_init)]\n",
    "    \n",
    "### Convert to geodataframe\n",
    "university = gpd.GeoDataFrame(university, geometry=[Point(xy) for xy in zip (university['LONGITUD'], university['LATITUDE'])])\n",
    "\n",
    "### Filter by institution size\n",
    "university = university[university['INSTSIZE']>1].reset_index(drop = True)\n",
    "\n",
    "### Keep only geometry type\n",
    "university = university[['geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keep only records within shapefile\n",
    "### this step optimizes the flagging of census tracts that contain the point of interest\n",
    "\n",
    "### Hospitals\n",
    "hospitals = hospitals[hospitals['geometry'].within(city_poly.loc[0, 'geometry'])].reset_index(drop = True)\n",
    "\n",
    "### Universities\n",
    "university = university[university['geometry'].within(city_poly.loc[0, 'geometry'])].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create anchor institution variable\n",
    "anchor_institutions = university.append(hospitals).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create half mile buffer\n",
    "\n",
    "### sets coordinate system to WGS84\n",
    "anchor_institutions.crs = {'init' :'epsg:4326'}\n",
    "\n",
    "### creates UTM projection\n",
    "### zone is defined under define city specific variables\n",
    "projection = '+proj=utm +zone='+zone+', +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "\n",
    "### project to UTM coordinate system\n",
    "anchor_institutions_proj = anchor_institutions.to_crs(projection)\n",
    "\n",
    "### create buffer around anchor institution in meters\n",
    "anchor_institutions_buffer = anchor_institutions_proj.buffer(804.672)\n",
    "\n",
    "### convert buffer back to WGS84\n",
    "anchor_institutions_buffer_wgs = anchor_institutions_buffer.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check whether census tract contains hospital station\n",
    "### and create anchor institution flag\n",
    "city_shp['anchor_institution'] = city_shp.intersects(anchor_institutions_buffer_wgs.unary_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = city_shp.plot(column = 'anchor_institution')\n",
    "anchor_institutions_buffer_wgs.plot(ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Subsidized housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIHTC\n",
    "lihtc = pd.read_csv('LowIncome_Housing_Tax_Credit_Properties.csv')\n",
    "\n",
    "### Public housing\n",
    "pub_hous = pd.read_csv('Public_Housing_Buildings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to geodataframe\n",
    "lihtc = gpd.GeoDataFrame(lihtc, geometry=[Point(xy) for xy in zip (lihtc['X'], lihtc['Y'])])\n",
    "pub_hous = gpd.GeoDataFrame(pub_hous, geometry=[Point(xy) for xy in zip (pub_hous['X'], pub_hous['Y'])])\n",
    "\n",
    "### Clip point to only region of interest\n",
    "### LIHTC\n",
    "lihtc = lihtc[lihtc['geometry'].within(city_poly.loc[0, 'geometry'])].reset_index(drop = True)\n",
    "\n",
    "### Public housing\n",
    "pub_hous = pub_hous[pub_hous['geometry'].within(city_poly.loc[0, 'geometry'])].reset_index(drop = True)\n",
    "\n",
    "### merge datasets\n",
    "presence_ph_LIHTC = lihtc[['geometry']].append(pub_hous[['geometry']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check whether census tract contains public housing or LIHTC station\n",
    "### and create public housing flag\n",
    "city_shp['presence_ph_LIHTC'] = city_shp.intersects(presence_ph_LIHTC.unary_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = city_shp.plot(color = 'grey')\n",
    "city_shp.plot(ax = ax, column = 'presence_ph_LIHTC')\n",
    "presence_ph_LIHTC.plot(ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_shp['GEOID'] = city_shp['GEOID'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census = census.merge(city_shp[['GEOID', 'rail', 'anchor_institution', 'presence_ph_LIHTC', 'geometry']], right_on = 'GEOID', left_on = 'FIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census.to_csv(city_name+'_database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
